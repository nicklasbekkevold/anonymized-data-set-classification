{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# TDT05 Classification - Challenge 2\n",
    "\n",
    "## Team: 5_AI-gutta\n",
    "\n",
    "- Magnus Schjølberg     \n",
    "- Nicklas Bekkevold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Table of contents\n",
    "1. Table of contents\n",
    "2. Imports and loading the data \n",
    "3. Exploratory Data Analysis (EDA)\n",
    "4. Feature Engineering\n",
    "5. Data Cleaning\n",
    "6. Model Choice and Training\n",
    "7. Prediction\n",
    "8. Model Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-11T09:48:35.033793Z",
     "iopub.status.busy": "2021-10-11T09:48:35.032907Z",
     "iopub.status.idle": "2021-10-11T09:48:35.040299Z",
     "shell.execute_reply": "2021-10-11T09:48:35.039452Z",
     "shell.execute_reply.started": "2021-10-11T09:48:35.033754Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor, metrics\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split, KFold\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "print('Tensorflow version:', tf.__version__)\n",
    "\n",
    "# Config\n",
    "rcParams['figure.figsize'] = 20, 10  # Make plots bigger\n",
    "RANDOM_SEED = 42  # For reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2021-10-11T09:48:35.209167Z",
     "iopub.status.busy": "2021-10-11T09:48:35.208320Z",
     "iopub.status.idle": "2021-10-11T09:48:35.796182Z",
     "shell.execute_reply": "2021-10-11T09:48:35.795231Z",
     "shell.execute_reply.started": "2021-10-11T09:48:35.209116Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "training_data = pd.read_csv('../input/tdt05-2021-challenge-2/challenge2_train.csv', index_col=0)\n",
    "X_test = pd.read_csv('../input/tdt05-2021-challenge-2/challenge2_test.csv', index_col=0)\n",
    "\n",
    "print('Training shape:', training_data.shape)\n",
    "print('Test shape:', X_test.shape)\n",
    "summary_statistics = pd.DataFrame(\n",
    "    {\n",
    "        'dtype': training_data.dtypes,\n",
    "        'categories': training_data.nunique(),\n",
    "        '#nan': training_data.isnull().sum(axis = 0),\n",
    "    }, \n",
    "    index=training_data.columns,\n",
    ")\n",
    "\n",
    "print(summary_statistics)\n",
    "training_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-11T09:48:35.798555Z",
     "iopub.status.busy": "2021-10-11T09:48:35.798250Z",
     "iopub.status.idle": "2021-10-11T09:48:35.910507Z",
     "shell.execute_reply": "2021-10-11T09:48:35.909440Z",
     "shell.execute_reply.started": "2021-10-11T09:48:35.798517Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.pie(\n",
    "    training_data['target'].value_counts(), \n",
    "    colors=sns.color_palette('pastel')[0:2], \n",
    "    labels=['0', '1'],\n",
    "    autopct='%.0f%%',\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target value is 0 or 1, indicating that the task to solve is binary classification.  \n",
    "There is a lot more of the 0-class (81%) than the 1-class (19%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-11T09:48:35.912692Z",
     "iopub.status.busy": "2021-10-11T09:48:35.912341Z",
     "iopub.status.idle": "2021-10-11T09:48:43.924315Z",
     "shell.execute_reply": "2021-10-11T09:48:43.923389Z",
     "shell.execute_reply.started": "2021-10-11T09:48:35.912652Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.catplot(data=training_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to see that several of the features has \"-1\" as a value. This might be an explicit na / null-value set by a user or something similar.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot feature distributions\n",
    "\n",
    "We plot all the features to get a feel for the different features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all features to see their shape\n",
    "fig, axes = plt.subplots(5, 6)\n",
    "\n",
    "for i, feature in enumerate(list(training_data.columns)):\n",
    "    ax = sns.histplot(training_data[feature], ax=axes[i // 6][i % 6])\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Feature description\n",
    "\n",
    "Each feature is listed below with some example values and our notes.  \n",
    "We have also grouped each feature into categories based on similarities to make preprocessing easier.  \n",
    "(We also used Kaggle's built in data explorer and Excel to take a look at the features.)\n",
    "\n",
    "| Feature | Example | Our feature \"category\" | Note |\n",
    "| :-- | :-- | :-- | :-- |\n",
    "| target | 0 or 1 | boolean | This means that the problem is a binary classification |\n",
    "| f0 | 0.0 or 1.0 | boolean | Could be turned into a categorical feature |\n",
    "| f1 | gL, Rj, In, ... | categorical | 173 unique two letter pairs, could be country codes or something similar |\n",
    "| f2 | a, b, ..., f | alphabetic | letters a-f |\n",
    "| f3 | 1.0, 2.0 or 3.0 | ordinal | ordinal of some kind, should not be converted to categorical |\n",
    "| f4 | A or B | boolean | mostly As (=88%) |\n",
    "| f5 | -1, 0, ..., 11 | ordinal | weird distribution, many null values |\n",
    "| f6 | 0 or 10 | binary | Binary - could be turned into a categorical feature |\n",
    "| f7 | 1, 2, ..., 6 | ordinal | Ordinal of some kind, should not be converted to categorical |\n",
    "| f8 | '96ae67d3e', 'a5adff44e' | hexadecimal | Looks like hexadecimal, might be a hash of some kind |\n",
    "| f9 | red, white, black, green, yellow | categorical | five colors |\n",
    "| f10 | A, B, ..., Z | alphabetic | Could be converted to integers to get ordinal property |\n",
    "| f11 | 139681, 140242 | interval |\n",
    "| f12 | '1c756c04a', '5d1ac7760' | hexadecimal | Looks like hexadecimal, might be a hash of some kind |\n",
    "| f13 | a, b, ..., o | alphabetic | Could be converted to integers to get ordinal property |\n",
    "| f14 | '168e51823', '558613041' | hexadecimal | Looks like hexadecimal, might be a hash of some kind |\n",
    "| f15 | '7861df0a8', '1d88b0a79' | hexadecimal | Looks like hexadecimal, might be a hash of some kind |\n",
    "| f16 | 0, 10, ..., 120 | ordinal | Ordinal of some kind, should not be converted to categorical |\n",
    "| f17 | 858314945, 616742978 | interval |  |\n",
    "| f18 | A, B, ..., F | alphabetic | Could be converted to integers to get ordinal property |\n",
    "| f19 | 0, 1, ..., 6 | ordinal | Ordinal of some kind, should not be converted to categorical |\n",
    "| f20 | 00, 01, 10 or 11 | binary | Looks like a binary number. Could be converted to 0, 1, 2 and 3 |\n",
    "| f21 | 0, 10, ..., 70 | ordinal | Ordinal of some kind, should not be converted to categorical |\n",
    "| f22 | R, G or B | categorical | Could it be a coincidence that it is RGB? |\n",
    "| f23 | '328b0cf4e', '0c67fcbbd' | hexadecimal | Looks like hexadecimal, might be a hash of some kind |\n",
    "| f24 | 834041366, 686021137 | interval |  |\n",
    "| f25 | T or F | Boolean | Boolean - (T)rue or (F)alse |\n",
    "| f26 | N or S | Boolean | Boolean - (N)orth or (S)outh? |\n",
    "| f27 | 1, 2, ..., 4 | ordinal | Ordinal of some kind, should not be converted to categorical |\n",
    "| f28 | 141855, 139805 | interval |  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature mean distribution\n",
    "\n",
    "Check if there is any noticeable groupings in the feature means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean per feature \n",
    "training_data.mean(axis=0).sort_values().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature f11 and f28 have similar mean values. Let's see if they are correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation plot\n",
    "\n",
    "See if there are any interesting patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlations using the pearson method\n",
    "categorical_encoder = lambda x : pd.factorize(x)[0]  # Encodes categorical data for correlation plot \n",
    "encoded_training_data = training_data.apply(categorical_encoder)\n",
    "\n",
    "correlation = encoded_training_data.corr(method='pearson', min_periods=1)\n",
    "correlation.style.background_gradient(cmap='viridis').set_precision(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f11 and f28 does not have a significant correlation, but they might be related in some other way.  \n",
    "However, f20 and f27 have a very high correlation using the *pearson* method. The other feature pairs seem quite uncorrelated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the dataset is shuffled\n",
    "\n",
    "To check if the dataset is shuffled or have any trends we use the *rolling mean* for three features: f17, f24, and f28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make rolling mean plot\n",
    "features = ['f17', 'f24', 'f28']\n",
    "fig, axes = plt.subplots(len(features), 1)\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    training_data[feature].notna().rolling(window=500).mean().plot(ax=axes[i], legend=feature)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data seems to be randomly distributed, i.e. it does not show any noticeable patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the hexadecimal features\n",
    "\n",
    "The features labeled:\n",
    "- f8 \n",
    "- f12 \n",
    "- f14 \n",
    "- f15 \n",
    "- f23\n",
    "\n",
    "All seem to be hexadecimal. These might just be an id of sorts, or anonymized words, or they can be the hex representation of a number.  \n",
    "An idea can be to covert these into decimal and see if they are important somehow.\n",
    "\n",
    "Or check if the number of categories increases significantly when we add the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2021-10-11T09:48:45.790673Z",
     "iopub.status.busy": "2021-10-11T09:48:45.790425Z",
     "iopub.status.idle": "2021-10-11T09:48:46.051433Z",
     "shell.execute_reply": "2021-10-11T09:48:46.050474Z",
     "shell.execute_reply.started": "2021-10-11T09:48:45.790644Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check if the number of categories increases when we add the test data \n",
    "hexadecimal_features = ['f8', 'f12', 'f14', 'f15', 'f23']\n",
    "merged_datasets = pd.concat([training_data, X_test])[hexadecimal_features]\n",
    "\n",
    "merged_statistics = pd.DataFrame(\n",
    "    {\n",
    "        '#categories before merge': training_data.nunique(), \n",
    "        '#categories after merge': merged_datasets.nunique(),\n",
    "        'increase': merged_datasets.nunique() - training_data.nunique(),\n",
    "    }, \n",
    "    index=merged_datasets.columns,\n",
    ")\n",
    "\n",
    "print(merged_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "By encoding some of the categorical features, we might be able to see some patterns more clearly and improve the correlation analysis.  \n",
    "It might also be useful for predictors who only accepts numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the data to have both versions available\n",
    "encoded_training_data = training_data.copy()\n",
    "X_test_encoded = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert hexadecimal values to decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-11T09:48:43.926565Z",
     "iopub.status.busy": "2021-10-11T09:48:43.926267Z",
     "iopub.status.idle": "2021-10-11T09:48:43.933083Z",
     "shell.execute_reply": "2021-10-11T09:48:43.932213Z",
     "shell.execute_reply.started": "2021-10-11T09:48:43.926534Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert hexadecimal numbers to decimal\n",
    "hexadecimal_features = ['f8', 'f12', 'f14', 'f15', 'f23']\n",
    "\n",
    "hexadecimal_to_decimal = lambda hexadecimal: int(hexadecimal, base=16)\n",
    "encoded_training_data[hexadecimal_features] = encoded_training_data[hexadecimal_features].applymap(hexadecimal_to_decimal, na_action='ignore')\n",
    "X_test_encoded[hexadecimal_features] = X_test_encoded[hexadecimal_features].applymap(hexadecimal_to_decimal, na_action='ignore')\n",
    "\n",
    "encoded_training_data[hexadecimal_features].astype(float)\n",
    "encoded_training_data[hexadecimal_features].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution\n",
    "# See if there are some patterns in the hexadecimal features if converted to decimal numbers \n",
    "fig, axes = plt.subplots(1, len(hexadecimal_features))\n",
    "\n",
    "for i, feature in enumerate(hexadecimal_features):\n",
    "    ax = sns.histplot(encoded_training_data[feature], ax=axes[i])\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert binary numbers to decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-11T09:48:43.936084Z",
     "iopub.status.busy": "2021-10-11T09:48:43.935888Z",
     "iopub.status.idle": "2021-10-11T09:48:43.948314Z",
     "shell.execute_reply": "2021-10-11T09:48:43.947555Z",
     "shell.execute_reply.started": "2021-10-11T09:48:43.936061Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert binary numbers to decimal\n",
    "binary_features = ['f20']  # 'f6' is automatically converted by pandas\n",
    "\n",
    "binary_to_decimal = { \n",
    "    0.0: 0, \n",
    "    1.0: 1,\n",
    "    10.0: 2,\n",
    "    11.0: 3 \n",
    "}\n",
    "\n",
    "encoded_training_data[binary_features[0]] = encoded_training_data[binary_features[0]].map(binary_to_decimal, na_action='ignore')\n",
    "X_test_encoded[binary_features[0]] = X_test_encoded[binary_features[0]].map(binary_to_decimal, na_action='ignore')\n",
    "\n",
    "encoded_training_data[binary_features[0]].astype(float)\n",
    "X_test_encoded[binary_features[0]].astype(float)\n",
    "\n",
    "print(encoded_training_data[binary_features[0]].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert alphabetic characters to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert alphabetic characters to numbers: A=0, B=1, ..., Z=25\n",
    "alphabetic_features = ['f2', 'f10', 'f13', 'f18']\n",
    "\n",
    "alphabetic_to_ordinal = lambda c: ord(c.lower()) - 97\n",
    "encoded_training_data[alphabetic_features] = encoded_training_data[alphabetic_features].applymap(alphabetic_to_ordinal, na_action='ignore')\n",
    "X_test_encoded[alphabetic_features] = X_test_encoded[alphabetic_features].applymap(alphabetic_to_ordinal, na_action='ignore')\n",
    "\n",
    "encoded_training_data[alphabetic_features].astype(float)\n",
    "X_test_encoded[alphabetic_features].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert boolean features to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert boolean features to 0.0 and 1.0\n",
    "boolean_mappings = {\n",
    "    'f4': {'B': 0.0, 'A': 1.0},\n",
    "    'f25': {'F': 0.0, 'T': 1.0},\n",
    "    'f26': {'S': 0.0, 'N': 1.0},\n",
    "}\n",
    "\n",
    "boolean_features = boolean_mappings.keys()\n",
    "\n",
    "for feature, mapping in boolean_mappings.items():\n",
    "    encoded_training_data[feature] = encoded_training_data[feature].map(mapping, na_action='ignore')\n",
    "    X_test_encoded[feature] = X_test_encoded[feature].map(mapping, na_action='ignore')\n",
    "    \n",
    "    encoded_training_data[feature].astype(float)\n",
    "    X_test_encoded[feature].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decrement f3, f7, and f27 by one\n",
    "\n",
    "- f3 ranges from 1-3\n",
    "- f4 ranges from 1-6\n",
    "- f27 ranges from 1-4\n",
    "\n",
    "We shift their scales down by one to make then more comparable to other features which start at 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decrement features by one\n",
    "features = ['f3', 'f7', 'f27']\n",
    "decrement_by_one = lambda number: number - 1\n",
    "\n",
    "encoded_training_data[features] = encoded_training_data[features].applymap(decrement_by_one, na_action='ignore')\n",
    "encoded_training_data[features].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation plot for encoded training data\n",
    "\n",
    "Plot with encoded values to look for correlation across *nearly* all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-11T09:48:43.950277Z",
     "iopub.status.busy": "2021-10-11T09:48:43.949574Z",
     "iopub.status.idle": "2021-10-11T09:48:45.493217Z",
     "shell.execute_reply": "2021-10-11T09:48:45.492426Z",
     "shell.execute_reply.started": "2021-10-11T09:48:43.950235Z"
    }
   },
   "outputs": [],
   "source": [
    "# Correlation plot\n",
    "correlation = encoded_training_data.corr()\n",
    "sns.heatmap(correlation, annot=True, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f20 and f27 have a very high correlation, which might indicate that one of them could be dropped for generality.  \n",
    "We will do some feature pair plots to look for further patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the five most correlated feature pairs\n",
    "keep = np.triu(np.ones(correlation.shape), k=1).astype('bool').reshape(correlation.size)\n",
    "correlation = correlation.abs().stack()[keep]  # Only use the lower triangular part of the correlation matrix\n",
    "correlation = correlation.sort_values(ascending=False)[0:5]  # Only keep top 5\n",
    "correlation = pd.DataFrame(correlation)\n",
    "print('The five most correlated features:', correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(correlation))\n",
    "\n",
    "for i, (feature_x, feature_y) in enumerate(correlation.index):\n",
    "    ax = sns.scatterplot(data=training_data, x=feature_x, y=feature_y, ax=axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation plot shows that **f20** and **f27** have a very high correlation. One of these could be dropped to increase generality (and reduce the computational cost). \n",
    "\n",
    "The feature pairs (**f5**, **f24**) and (**f5**, **f17**) tend to increase together\n",
    "We can also see that the features **f3** and **f16** have a higher correlation with the target value than others, and that we should probably focus our efforts on these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Data cleaning\n",
    "Data cleaning consists of the following steps\n",
    "- Removal of unwanted observations\n",
    "- Fixing structural errors\n",
    "- Managing unwanted outliers\n",
    "- Handeling missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2021-10-11T09:48:46.055191Z",
     "iopub.status.busy": "2021-10-11T09:48:46.054966Z",
     "iopub.status.idle": "2021-10-11T09:48:46.175202Z",
     "shell.execute_reply": "2021-10-11T09:48:46.174454Z",
     "shell.execute_reply.started": "2021-10-11T09:48:46.055167Z"
    }
   },
   "outputs": [],
   "source": [
    "# Duplicate detection\n",
    "training_data.duplicated().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No duplicate rows in the data set, so no need to remove any rows either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier detection and removal\n",
    "\n",
    "The IQR method is used in this project for outlier detection and removal.  \n",
    "\n",
    "We decided to only use the interval features (f11, f17, f24, and f28) for outlier detection and removal.  \n",
    "This is because these features are numerical, have many unique values, and they all follow a clear distribution.  \n",
    "\n",
    "The next feature with the most unique values after f24 is **f8**, which is a categorical feature (and have a lot fewer unique values): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the features with the most categories\n",
    "summary_statistics['categories'].sort_values(ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2021-10-11T09:48:46.176677Z",
     "iopub.status.busy": "2021-10-11T09:48:46.176445Z",
     "iopub.status.idle": "2021-10-11T09:48:48.464610Z",
     "shell.execute_reply": "2021-10-11T09:48:48.463701Z",
     "shell.execute_reply.started": "2021-10-11T09:48:46.176653Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot violin plot, box plot and histogram to show the distributions and highlight outliers\n",
    "interval_features = ['f11', 'f17', 'f24', 'f28']\n",
    "fig, axes = plt.subplots(3, len(interval_features))\n",
    "\n",
    "for i, feature in enumerate(interval_features):\n",
    "    ax = sns.violinplot(x=training_data[feature], ax=axes[0][i])\n",
    "    ax = sns.boxplot(x=training_data[feature], ax=axes[1][i])\n",
    "    ax = sns.histplot(training_data[feature], ax=axes[2][i])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-11T09:48:48.466659Z",
     "iopub.status.busy": "2021-10-11T09:48:48.466116Z",
     "iopub.status.idle": "2021-10-11T09:48:48.795452Z",
     "shell.execute_reply": "2021-10-11T09:48:48.794866Z",
     "shell.execute_reply.started": "2021-10-11T09:48:48.466616Z"
    }
   },
   "outputs": [],
   "source": [
    "# Outlier removal by interquartile range (IQR) method\n",
    "Q1 = training_data[interval_features].quantile(0.25)\n",
    "Q3 = training_data[interval_features].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Only remove the \"right tail\" from the features to keep the -1 value\n",
    "initial_row_count = training_data.shape[0]\n",
    "training_data = training_data[~(training_data > (Q3 + 1.5 * IQR)).any(axis=1)]\n",
    "encoded_training_data = encoded_training_data[~(encoded_training_data > (Q3 + 1.5 * IQR)).any(axis=1)]  # Also do this step for the encoded data\n",
    "rows_removed = initial_row_count - training_data.shape[0]\n",
    "print(f'Rows removed: {rows_removed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## Dealing with missing values\n",
    "\n",
    "Different predictors have different schemes for handling missing data.  \n",
    "In this project we have focused on XGBoost and CatBoost primarily.\n",
    "\n",
    "XGBoost and CatBoost missing values are handled separately before training.\n",
    "- XGBoost: missing values should be zero\n",
    "- CatBoost: missing values should be way off the distribution, -999 seems to be a popular choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition the data into the convectional 'X' and 'y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2021-10-11T09:48:49.167792Z",
     "iopub.status.busy": "2021-10-11T09:48:49.167426Z",
     "iopub.status.idle": "2021-10-11T09:48:49.205377Z",
     "shell.execute_reply": "2021-10-11T09:48:49.204453Z",
     "shell.execute_reply.started": "2021-10-11T09:48:49.167752Z"
    }
   },
   "outputs": [],
   "source": [
    "# Partition the data\n",
    "X = training_data.drop(columns=['target']).copy()\n",
    "X_encoded = encoded_training_data.drop(columns=['target']).copy()\n",
    "\n",
    "y = training_data[['target']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Choice and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "\n",
    "Useful functions for converting back and forth between feature names (e.g. 'f10') to indices (e.g. 10) and recomputing these if any features has been dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to go from, e.g. 'f10' to 10\n",
    "def features_to_indices(features):\n",
    "    return [int(column.replace('f', '')) for column in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to go from, e.g. 10 to 'f10\n",
    "def indices_to_features(indices):\n",
    "    return [f'f{index}' for index in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-11T09:48:49.451212Z",
     "iopub.status.busy": "2021-10-11T09:48:49.450927Z",
     "iopub.status.idle": "2021-10-11T09:48:49.457259Z",
     "shell.execute_reply": "2021-10-11T09:48:49.456346Z",
     "shell.execute_reply.started": "2021-10-11T09:48:49.451184Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function for computing the new indices after dropping certain columns\n",
    "def indices_after_droping(indices, dropped_columns):\n",
    "    updated_indices = list(indices)\n",
    "    dropped_indices = features_to_indices(dropped_columns)\n",
    "    \n",
    "    for i, old_index in enumerate(indices):\n",
    "        for column_index in dropped_indices:\n",
    "            if old_index >= column_index:\n",
    "                updated_indices[i] -= 1\n",
    "\n",
    "    return updated_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## CatBoost\n",
    "\n",
    "CatBoost expects missing values to be set to a value which is clearly off the distribution of the feature. -999 seems to be a normal choice.  \n",
    "CatBoost also expects categorical data to be strings. The indices of these also needs to be specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost specific preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-11T09:48:49.206910Z",
     "iopub.status.busy": "2021-10-11T09:48:49.206683Z",
     "iopub.status.idle": "2021-10-11T09:48:49.449752Z",
     "shell.execute_reply": "2021-10-11T09:48:49.448834Z",
     "shell.execute_reply.started": "2021-10-11T09:48:49.206886Z"
    }
   },
   "outputs": [],
   "source": [
    "# CatBoost specific preprocessing\n",
    "\n",
    "# Imputing\n",
    "fill_value = -999 \n",
    "\n",
    "# Make new data sets for CatBoost\n",
    "X_catboost = X.fillna(fill_value)\n",
    "X_test_catboost = X_test.fillna(fill_value)\n",
    "\n",
    "X_encoded_catboost = X_encoded.fillna(fill_value)\n",
    "X_encoded_test_catboost = X_test_encoded.fillna(fill_value)\n",
    "\n",
    "missing_values = pd.DataFrame(\n",
    "    {\n",
    "        '#nan_train': X_catboost.isnull().sum(axis = 0),\n",
    "        '#nan_test': X_test_catboost.isnull().sum(axis = 0),\n",
    "    }, \n",
    "    index=X_catboost.columns,\n",
    ")\n",
    "\n",
    "# Check if there are any missing values left\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting numerical features to categorical / \"strings\"\n",
    "categorical_numerical_features = ['f0', 'f6', 'f20']\n",
    "\n",
    "X_catboost[categorical_numerical_features] = X_catboost[categorical_numerical_features].astype(str)\n",
    "\n",
    "X_test_catboost[categorical_numerical_features] = X_test_catboost[categorical_numerical_features].astype(str)\n",
    "\n",
    "# f5 and f28 have many missing values and could be dropped for generality\n",
    "catboost_dropped_features = ['f13']\n",
    "X_catboost.drop(catboost_dropped_features, axis=1, inplace=True)\n",
    "X_test_catboost.drop(catboost_dropped_features, axis=1, inplace=True)\n",
    "\n",
    "X_encoded_catboost.drop(catboost_dropped_features, axis=1, inplace=True)\n",
    "X_encoded_test_catboost.drop(catboost_dropped_features, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features_indices = [0, 1, 6, 9, 22]\n",
    "categorical_features_indices += features_to_indices([*hexadecimal_features, *alphabetic_features, *binary_features, *boolean_features])\n",
    "\n",
    "categorical_features_indices = sorted(categorical_features_indices)\n",
    "print('Categorical features:', categorical_features_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-11T09:48:49.458727Z",
     "iopub.status.busy": "2021-10-11T09:48:49.458483Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_train, X_validation, y_train, y_validation = train_test_split(X_catboost, y, train_size=0.8, random_state=42)\n",
    "\n",
    "catboost_dropped_indices = features_to_indices(catboost_dropped_features)\n",
    "categorical_features_indices = [index for index in categorical_features_indices if index not in catboost_dropped_indices]\n",
    "categorical_features_indices_after_dropping = indices_after_droping(categorical_features_indices, catboost_dropped_features)\n",
    "print('Categorical features after dropping:', categorical_features_indices_after_dropping)\n",
    "\n",
    "catboost_model = CatBoostClassifier(\n",
    "    custom_loss=[metrics.Accuracy()],\n",
    "    random_seed=RANDOM_SEED,\n",
    "    logging_level='Silent'\n",
    ")\n",
    "\n",
    "catboost_model.fit(\n",
    "    X_catboost, y,\n",
    "    cat_features=categorical_features_indices_after_dropping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_, accuracy), (_, log_loss) = catboost_model.get_best_score()['learn'].items()\n",
    "print(f'CatBoost classifier accuracy: {accuracy * 100:.2f}%')\n",
    "print(f'CatBoost classifier log loss: {log_loss * 100:.2f}%')\n",
    "\n",
    "feature_importance = pd.DataFrame({'feature_importance': catboost_model.get_feature_importance()}, index=X_catboost.columns)\n",
    "feature_importance.sort_values(by='feature_importance', ascending=False, inplace=True)\n",
    "sns.barplot(data=feature_importance, x='feature_importance', y=feature_importance.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training CatBoost regressor\n",
    "\n",
    "We would like to se if using regression instead of classification could be beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_regressor = CatBoostRegressor(\n",
    "    random_seed=RANDOM_SEED,\n",
    "    logging_level='Silent'\n",
    ")\n",
    "\n",
    "catboost_regressor.fit(\n",
    "    X_catboost, y,\n",
    "    cat_features=categorical_features_indices_after_dropping,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = catboost_regressor.get_best_score()['learn']['RMSE']\n",
    "print(f'CatBoost regressor RMSE: {RMSE * 100:.2f}%')\n",
    "\n",
    "feature_importance = pd.DataFrame({'feature_importance': catboost_regressor.get_feature_importance()}, index=X_catboost.columns)\n",
    "feature_importance.sort_values(by='feature_importance', ascending=False, inplace=True)\n",
    "sns.barplot(data=feature_importance, x='feature_importance', y=feature_importance.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "XGBoost expects all features to be numerical, as opposed to CatBoost which requires categorical features to be strings.  \n",
    "We use one-hot encoding for the categorical features which has not been converted to numbers in any prior preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost specific preprocessing\n",
    "categorical_features = indices_to_features(categorical_features_indices_after_dropping)\n",
    "print(categorical_features_indices)\n",
    "\n",
    "# One-hot encode all categorical features\n",
    "X_xgboost = pd.get_dummies(X, columns=categorical_features)\n",
    "X_test_xgboost = pd.get_dummies(X_test, columns=categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model = XGBClassifier()\n",
    "xgboost_model.fit(X_xgboost, np.ravel(y))\n",
    "\n",
    "predictions = xgboost_model.predict(X_xgboost)\n",
    "accuracy = accuracy_score(y, predictions)\n",
    "print('Accuracy: %.2f%%' % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "We try to stack the regular CatBoost classifier, the encoded CatBoost classifier and XGBoost together to see if we can improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_train(name, model, X, y, test, k_fold=KFold(n_splits=10)):\n",
    "\tprint(f'Training model: {name} ({model.__class__.__name__}) with {k_fold.get_n_splits()} splits.')\n",
    "\n",
    "\tnew_X = pd.DataFrame(columns=[name])\n",
    "\n",
    "\tfor train_ix, test_ix in k_fold.split(X):\t\n",
    "\t\ttrain_X, test_X = X.iloc[train_ix], X.iloc[test_ix]\n",
    "\t\ttrain_y, _ = y.iloc[train_ix], y.iloc[test_ix]\n",
    "\n",
    "\t\tmodel.fit(train_X, train_y)\n",
    "\n",
    "\t\tbatch_prediction = model.predict(test_X, prediction_type='Probability')[:,1]\n",
    "\n",
    "\t\tnew_X = new_X.append(pd.DataFrame(batch_prediction, columns=[name]), ignore_index=True)\n",
    "\n",
    "\tprediction = model.predict(test, prediction_type='Probability')[:,1]\n",
    "\treturn new_X, pd.DataFrame({name: prediction})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features_indices_encoded = [1, 9, 22]\n",
    "\n",
    "print('Categorical features (encoded):', categorical_features_indices_encoded)\n",
    "\n",
    "\n",
    "catboost_dropped_indices = features_to_indices(catboost_dropped_features)\n",
    "categorical_features_indices_encoded = [index for index in categorical_features_indices_encoded if index not in catboost_dropped_indices]\n",
    "categorical_features_indices_encoded_after_dropping = indices_after_droping(categorical_features_indices_encoded, catboost_dropped_features)\n",
    "\n",
    "categorical_features_indices = sorted(categorical_features_indices_encoded_after_dropping)\n",
    "print('Categorical features after dropping (encoded):', categorical_features_indices_encoded_after_dropping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking several CatBoosts together\n",
    "k_fold = KFold(n_splits=10)\n",
    "\n",
    "models = [\n",
    "    (\n",
    "        'cb_normal',\n",
    "        CatBoostClassifier(\n",
    "            cat_features=categorical_features_indices_after_dropping,\n",
    "            random_seed=RANDOM_SEED,\n",
    "            logging_level='Silent'\n",
    "        ),\n",
    "        X_catboost,\n",
    "        X_test_catboost\n",
    "    ),\n",
    "    (\n",
    "        'cb_encoded',\n",
    "        CatBoostClassifier(\n",
    "            cat_features=categorical_features_indices_encoded_after_dropping,\n",
    "            random_seed=RANDOM_SEED,\n",
    "            logging_level='Silent'\n",
    "        ),\n",
    "        X_encoded_catboost,\n",
    "        X_encoded_test_catboost,\n",
    "    ),\n",
    "    # (\n",
    "    #   'xgb'\n",
    "    #   XGBClassifier(),\n",
    "    #   X_xgboost,\n",
    "    #   X_test_xgboost,\n",
    "    # ),\n",
    "]\n",
    "\n",
    "results = [fold_train(name, model, data_set.head(10), y.head(10), test_data) for name, model, data_set, test_data in models]\n",
    "models_X, models_test_X = list(map(list, zip(*results)))  # transposes the data\n",
    "\n",
    "X_stacking = pd.concat(models_X, axis=1)\n",
    "X_test_stacking = pd.concat(models_test_X, axis=1)\n",
    "\n",
    "print(X_stacking)\n",
    "print(X_test_stacking)\n",
    "\n",
    "stacking_model = LogisticRegression()\n",
    "\n",
    "stacking_model.fit(X_stacking, np.ravel(y.head(10)))\n",
    "\n",
    "accuracy = stacking_model.score(X_stacking, np.ravel(y.head(10)))\n",
    "print(f'Stacking accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Choice\n",
    "\n",
    "Based on the results above, we choose to go forth with **CatBoost** as our main predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning (CatBoost)\n",
    "It is important to do hyperparameter tuning when training a machine learning model as these can have a great impact on both training time and accuracy. Doing proper hyperparameter tuning can help avoid local minima/maxima in both accuracy and loss. \n",
    "\n",
    "We attempted to avoid overfitting the model by using earlystopping and a validation dataset. Earlystopping is a technique wherein the model is tested on some unseen validation data continuously during training. The model is not trained on the validation data, and thus it is possible to identify when the model stops being more accurate for _unseen_ data, even though it may still be improving for the data it is itself trained on. A technique that often performs better than this however, is to use K-fold cross validation, the model switches between what data is used as training and validation data continuously, in order to utilize more of the available data for training.\n",
    "\n",
    "For the CatBoost predictor, we therefore use a randomized search with cross-validation in order to try to find some \"areas\" of potential optimal settings.  \n",
    "There are very many potential hyperparameters that can be tuned, but we have decided to focus only on those who have been considered the most important in other similar experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all parameters for CatBoost classifier\n",
    "CatBoostClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_model = CatBoostClassifier(\n",
    "    cat_features=categorical_features_indices_after_dropping,\n",
    "    random_seed=RANDOM_SEED,\n",
    "    logging_level='Silent'\n",
    ")\n",
    "\n",
    "hyperparameters = {\n",
    "    'iterations': [80, 100, 120, 140, 160],\n",
    "    'learning_rate': [0.03, 0.1, 0.2],\n",
    "    'depth': [2, 4, 6, 8],\n",
    "    'l2_leaf_reg': [0.2, 0.5, 1, 1.5, 2, 2.5, 3]\n",
    "}\n",
    "\n",
    "catboost_hyperparameters = RandomizedSearchCV(\n",
    "    estimator=catboost_model, \n",
    "    param_distributions=hyperparameters, \n",
    "    n_iter=30, \n",
    "    scoring='accuracy',\n",
    "    cv=3, \n",
    "    verbose=2, \n",
    "    random_state=42, \n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "catboost_hyperparameters.fit(X_catboost, y, use_best_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then list what hyperparameters gave the best results in the randomized search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_hyperparameters.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Predictions\n",
    "\n",
    "Make predictions from `X_test_catboost` (test data transformed for CatBoost) and save to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost\n",
    "y_pred = catboost_hyperparameters.predict_proba(X_test_catboost)\n",
    "predictions = pd.DataFrame({'id': X_test_catboost.index, 'target': y_pred[:,1]})\n",
    "\n",
    "predictions.to_csv('../output/catboost.txt', index=False)\n",
    "\n",
    "\n",
    "# XGBoost\n",
    "y_pred = xgboost_model.predict_proba(X_test_xgboost)\n",
    "predictions = pd.DataFrame({'id': X_test_xgboost.index, 'target': y_pred[:,1]})\n",
    "\n",
    "predictions.to_csv('../output/xgboost.txt', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we tried to do hyperparameter tuning, we could unfortunately not identify any particular configuration of hyperparameters in our search that performed any better than the default parameters that Catboost chooses out of the box. We therefore decided to use only out of the box configurations for Catboost, as any further attempts to improve the model with typical techniques only worsened the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP values / feature importance\n",
    "In order to explain the model, we are using a SHapley Additive exPlanations (SHAP) plot to assign an importance value to each feature for a given prediction. By using a summarizing plot for all features, we obtain a feature importance plot. In this plot, red values represent an increase in feature value while blue values represent a decrease. When a value has a negative SHAP value (to the left of 0.0) this represents a decrease in target value, while a positive SHAP value represents an increase in target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_feature_importance = catboost_model.get_feature_importance()\n",
    "explainer = shap.TreeExplainer(catboost_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test, max_display=X_test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the features in the dataset are gray in this plot, as they are not numeric features with defined «high» and «low» values. Some of them, like f13, could be converted to numeric in order to get an ordinal property and thus get further information (through coloring) in this plot. However, in the feature engineering phase, ww discovered that converting these properties to numeric only worsened the accuracy of the model, and this is therefore left out of this plot as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(catboost_model, X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By observing the output of the confusion matrix, we can conclude with some findings:\n",
    "The model mostly predicts the correct value for rows with a 0 as the target, with only a marginal amount of errors. For rows with 1 the target however, the model actually has an error rate of almost 80%. This is not that surprising as the dataset is very imbalanced, with a 80/20 split of 0 and 1 valued targets respectively, as we identified in the EDA section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: things that should be cleaned up / removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.dependence_plot(\"f2\", shap_values, X_catboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # confusion matrix\n",
    "# from sklearn.metrics import ConfusionMatrixDisplay\n",
    "# ConfusionMatrixDisplay"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37c8a2de656be8ca1d97a9c7d50c80471430111670512576e74e7e519fff4ab7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
